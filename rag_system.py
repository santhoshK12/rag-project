# -*- coding: utf-8 -*-
"""Rag system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XIjq2MR8Do1brLZS59Kl3t_O9GHIYRYy
"""

!pip install --force-reinstall langchain
!pip install -U langchain-community langchain-openai openai faiss-cpu tiktoken python-dotenv

OPENAI_API_KEY=""

import os
from dotenv import load_dotenv
load_dotenv()
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
#

from google.colab import files
uploaded= files.upload()

from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
import os

# The 'uploaded' variable contains the content of the files uploaded via files.upload()
# We will manually create Document objects from the uploaded files.

docs = []
for filename, content in uploaded.items():
    # Ensure only text files are processed if there are other types
    if filename.endswith('.txt'):
        # Decode content, assuming utf-8 for text files
        try:
            text_content = content.decode('utf-8')
            docs.append(Document(page_content=text_content, metadata={'source': filename}))
        except UnicodeDecodeError:
            print(f"Warning: Could not decode {filename} as UTF-8. Skipping.")

print(f"Number of loaded documents: {len(docs)}")
if docs:
    print(docs[0].page_content[:300])
else:
    print("No documents were loaded.")

from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=200,

)
chunks= text_splitter.split_documents(docs)
print("Number of chunks :", len(chunks))
print(chunks[0].page_content)

!pip install  langchain langchain-community faiss-cpu tiktoken

from langchain_openai.embeddings import OpenAIEmbeddings
embedding_model= OpenAIEmbeddings()

from langchain_community.vectorstores import FAISS
vector_db=FAISS.from_documents(chunks, embedding_model)
print("FAISS INDEX BUILT!")

vector_db.save_local("faiss_index")
print("FAISS INDEX SAVED!")

vector_db = FAISS.load_local(
    "faiss_index",
    embedding_model,
    allow_dangerous_deserialization=True
)
print("FAISS index loaded successfully!")

retriever = vector_db.as_retriever(
    search_kwargs={"k":4}
)

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0)

# Prompt
prompt = ChatPromptTemplate.from_messages([
    ("system", "Use ONLY the following context to answer:\n\n{context}"),
    ("user", "{question}")
])

# Combine docs
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

document_chain = (
    {"context": format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# FINAL RAG CHAIN
qa_chain = retriever | document_chain

response = qa_chain.invoke("Hoe modern NLP wprks ?")
print(response)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile rag_system.py
# # Your entire RAG pipeline code here
# # Copy-paste all imports, loading, splitting, embeddings, FAISS, retriever, chain, etc.
# 
#